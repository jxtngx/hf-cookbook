{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5f8cc6",
   "metadata": {},
   "source": [
    "# Advanced RAG on Hugging Face Collections with LangChain, Cohere, and Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a769d66e",
   "metadata": {},
   "source": [
    "This demo shows how to perform advanced Retrieval Augmented Generation (RAG) on documents contained in a Hugging Face Collection.\n",
    "\n",
    "For an introduction to advanced RAG, you can check out this [other cookbook](https://huggingface.co/learn/cookbook/en/advanced_rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d1f9e-4ec2-4b43-be76-9a70b226af98",
   "metadata": {},
   "source": [
    "We will use the following tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa8f59",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>requirement</th>\n",
    "            <th>purpose</th>\n",
    "            <th>link</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>LangChain</td>\n",
    "            <td>LLM workflow framework</td>\n",
    "            <td><a href=\"https://python.langchain.com/v0.2/docs/introduction/\">docs</a></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Comet LLM</td>\n",
    "            <td>tracking LLM workflows</td>\n",
    "            <td><a href=\"https://www.comet.com/docs/v2/guides/comet-llm/quickstart/\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Unstructured</td>\n",
    "            <td>document processing</td>\n",
    "            <td><a href=\"https://docs.unstructured.io/welcome\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Llama 3 70B Instruct</td>\n",
    "            <td>synthesizer model</td>\n",
    "            <td><a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Cohere Reranker</td>\n",
    "            <td>reranking model</td>\n",
    "            <td><a href=\"https://cohere.com/rerank\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Hugging Face Hub API</td>\n",
    "            <td>interact with the HF Hub</td>\n",
    "            <td><a href=\"https://huggingface.co/docs/huggingface_hub/index\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Hugging Face Inference API</td>\n",
    "            <td>serverless inference for prototyping</td>\n",
    "            <td><a href=\"https://huggingface.co/docs/api-inference/index\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Weaviate</td>\n",
    "            <td>vector database</td>\n",
    "            <td><a href=\"https://huggingface.co/docs/api-inference/index\">docs</a></td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>python-dotenv</td>\n",
    "            <td>reading environment variables</td>\n",
    "            <td><a href=\"https://saurabh-kumar.com/python-dotenv/\">docs</a></td> \n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f738d0",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b929755",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>langchain</li>\n",
    "    <li>langchain-weaviate</li>\n",
    "    <li>langchain-cohere</li>\n",
    "    <li>langchain-huggingface</li>\n",
    "    <li>langchain-community</li>\n",
    "    <li>weaviate-client</li>\n",
    "    <li>comet-llm</li>\n",
    "    <li>huggingface-hub</li>\n",
    "    <li>unstructured[all-docs]</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f932c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain huggingface-hub comet-llm \"unstructured[all-docs]\" weaviate-client langchain-weaviate langchain-cohere langchain-huggingface langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24367ec",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad87911",
   "metadata": {},
   "source": [
    "We need to do the following:\n",
    "\n",
    "<ol>\n",
    "  <li>Create the <a href=\"https://huggingface.co/docs/hub/collections\">collection</a> on Hugging Face</li>\n",
    "  <li>Fetch the collection with the Hugging Face Hub API</li>\n",
    "  <li>Preprocess the documents contained in the collection with Unstructured</li>\n",
    "  <li>Use Weaviate and LangChain to create a vector store and retriever</li>\n",
    "  <li>Use Hugging Face's Inference API to synthesize an answer using Meta's Llama 3 70B Instruct</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e93b01",
   "metadata": {},
   "source": [
    "## API keys we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b155e",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><a href=\"https://www.comet.com/site/\">Comet</a></li>\n",
    "    <li><a href=\"https://unstructured.io/api-key-free\">Unstructured</a></li>\n",
    "    <li><a href=\"https://weaviate.io/\">Weaviate</a></li>  \n",
    "    <li><a href=\"https://cohere.com/\">Cohere</a></li>  \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec64f4",
   "metadata": {},
   "source": [
    "# Breaking down RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfe4ba",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8ba77",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094f494",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773160e0",
   "metadata": {},
   "source": [
    "### Vector search with Hierarchical Navigable Small Worlds (HNSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e1653",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3745d",
   "metadata": {},
   "source": [
    "### Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac843d4",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948b502",
   "metadata": {},
   "source": [
    "### Reranking models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6190a3d",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702effe9",
   "metadata": {},
   "source": [
    "## Synthesis aka Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407416b",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64864eee",
   "metadata": {},
   "source": [
    "# Combining LangChain and Comet LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc62b7",
   "metadata": {},
   "source": [
    "See the Comet LLM [docs](https://www.comet.com/docs/v2/guides/comet-llm/integrations/langchain/) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de4ee9",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a820f",
   "metadata": {},
   "source": [
    "## What is LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991484b9",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156ba51",
   "metadata": {},
   "source": [
    "## What is Comet LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d654b",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da967426",
   "metadata": {},
   "source": [
    "# The Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fc4f0",
   "metadata": {},
   "source": [
    "## Get the collection and download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import get_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8a1c8",
   "metadata": {},
   "source": [
    "ensure the data directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join(\".\", \"documents\")\n",
    "\n",
    "if not os.path.isdir(datadir):\n",
    "    os.mkdir(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b3858",
   "metadata": {},
   "source": [
    "get the collection's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a283474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data dir contents\n",
    "data = os.listdir(datadir)\n",
    "# get tool use paper collection\n",
    "collection = get_collection(\"jxtngx/tool-use-papers-664c6cd9cc9c64354af51e86\")\n",
    "# make arxiv urls\n",
    "urls = [\"\".join([\"https://arxiv.org/pdf/\", c.item_id]) for c in collection.items]\n",
    "\n",
    "# download files\n",
    "for url in urls:\n",
    "    docname = \"\".join([url.split('/')[-1], \".pdf\"])\n",
    "    if not os.path.exists(os.path.join(datadir, docname)):\n",
    "        os.system(f\"wget -O {os.path.join(datadir, docname)} {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b435a82",
   "metadata": {},
   "source": [
    "## Prep PDFs with Unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0a266",
   "metadata": {},
   "source": [
    "This section takes inspiration from the example in [Building RAG with Custom Unstructured Data](https://github.com/huggingface/cookbook/blob/main/notebooks/en/rag_with_unstructured_data.ipynb), by Maria Khalusova.\n",
    "\n",
    "See the sections titled `Unstructured data preprocessing` and `Chunking` of that example for a detailed walk through provided by Maria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from unstructured.ingest.connector.local import SimpleLocalConfig\n",
    "from unstructured.ingest.interfaces import PartitionConfig, ProcessorConfig, ReadConfig\n",
    "from unstructured.ingest.runner import LocalRunner\n",
    "from unstructured.staging.base import elements_from_json\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cell to reduce the amount of logs\n",
    "logger = logging.getLogger(\"unstructured.ingest\")\n",
    "\n",
    "if logger.root.handlers:\n",
    "    logger.root.removeHandler(logger.root.handlers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./local-ingest-output\"\n",
    "\n",
    "runner = LocalRunner(\n",
    "    processor_config=ProcessorConfig(\n",
    "        # logs verbosity\n",
    "        verbose=True,\n",
    "        # the local directory to store outputs\n",
    "        output_dir=output_path,\n",
    "        num_processes=2,\n",
    "        ),\n",
    "    read_config=ReadConfig(),\n",
    "    partition_config=PartitionConfig(\n",
    "        partition_by_api=True,\n",
    "        api_key=os.environ[\"UNSTRUCTURED_API_KEY\"],\n",
    "        ),\n",
    "    connector_config=SimpleLocalConfig(\n",
    "        input_path=\"./documents\",\n",
    "        # whether to get the documents recursively from given directory\n",
    "        recursive=False,\n",
    "        ),\n",
    "    )\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = []\n",
    "\n",
    "for filename in os.listdir(output_path):\n",
    "    filepath = os.path.join(output_path, filename)\n",
    "    elements.extend(elements_from_json(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ecc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_elements = chunk_by_title(elements,\n",
    "                                  # maximum for chunk size\n",
    "                                  max_characters=512,\n",
    "                                  # You can choose to combine consecutive elements that are too small\n",
    "                                  # e.g. individual list items\n",
    "                                  combine_text_under_n_chars=200,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for chunked_element in chunked_elements:\n",
    "    metadata = chunked_element.metadata.to_dict()\n",
    "    metadata[\"source\"] = metadata[\"filename\"]\n",
    "    del metadata[\"languages\"]\n",
    "    documents.append(Document(page_content=chunked_element.text, metadata=metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792eb455",
   "metadata": {},
   "source": [
    "## Create the vector store and retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cee8ed",
   "metadata": {},
   "source": [
    "See the [LangChain docs](https://python.langchain.com/v0.2/docs/integrations/text_embedding/huggingfacehub/) for more on the Hugging Face embeddings integration.\n",
    "\n",
    "See the [LangChain docs](https://python.langchain.com/v0.1/docs/integrations/vectorstores/weaviate/) for more on the Weaviate integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.auth import AuthApiKey\n",
    "\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_cohere import CohereRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c6edf",
   "metadata": {},
   "source": [
    "Connect to Weaviate Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WEAVIATE_URL\"),\n",
    "    auth_credentials=AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ[\"HF_INFERENCE_API_KEY\"], \n",
    "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
    ")\n",
    "vectorstore = WeaviateVectorStore.from_documents(documents, embeddings, client=weaviate_client)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank(cohere_api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f77edb",
   "metadata": {},
   "source": [
    "## Using Hugging Face Inference Endpoints for prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b27a9d",
   "metadata": {},
   "source": [
    "See the [LangChain docs](https://python.langchain.com/v0.2/docs/integrations/llms/huggingface_endpoint/) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59521061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    model_kwargs={\"max_length\": 128},\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=os.environ[\"HF_INFERENCE_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e89981",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=compression_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"What is chain of abstraction\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b542cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in wrap(result):\n",
    "    for token in sequence.split():\n",
    "        print(token, end=\" \", flush=True)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
